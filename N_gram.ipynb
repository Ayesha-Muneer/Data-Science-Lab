{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-gram.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayesha-Muneer/Data-Science-Lab/blob/master/N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UFeG4SjgXarU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a72f57a-9278-4fb9-f655-5b9a98293c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re, pprint, string\n",
        "import pandas as pd\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0z2L6mkCJw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')"
      ],
      "metadata": {
        "id": "eHy4jNo4YLAG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = pd.read_csv('corpus.csv')\n",
        "print(corpus)\n",
        "file_nl_removed = \"\"\n",
        "for line in corpus:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters\n",
        "\n",
        "print(file_p)"
      ],
      "metadata": {
        "id": "bsocShRCZfc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5cd3e0-b15b-4874-bf0f-17f30f1b8f93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  ... Unnamed: 21\n",
            "0   Stuning even for the non-gamer: This sound tra...  ...         NaN\n",
            "1   The best soundtrack ever to anything.: I'm rea...  ...         NaN\n",
            "2   \" Amazing!: This soundtrack is my favorite mus...  ...         NaN\n",
            "3   Excellent Soundtrack: I truly like this soundt...  ...         NaN\n",
            "4   \" Excellent Soundtrack: I truly like this soun...  ...         NaN\n",
            "5   \" Remember, Pull Your Jaw Off The Floor After ...  ...         NaN\n",
            "6   \" an absolute masterpiece: I am quite sure any...  ...         NaN\n",
            "7   \" Buyer beware: This is a self-published book,...  ...         NaN\n",
            "8   \" Glorious story: I loved Whisper of the wicke...  ...         NaN\n",
            "9   \" A FIVE STAR BOOK: I just finished reading Wh...  ...         NaN\n",
            "10  \" Disappointed Romanian!: This book in my opin...  ...         NaN\n",
            "11  \" Not the best: I bought both this and lonely ...  ...         NaN\n",
            "12   Good but received defective book: I bought th...  ...         NaN\n",
            "13  \" From The Label:: From Memphis, TN. comes The...  ...         NaN\n",
            "14  \" Either 1 or 5 Stars. Depends on how you look...  ...         NaN\n",
            "\n",
            "[15 rows x 22 columns]\n",
            "textlabelUnnamed 2Unnamed 3Unnamed 4Unnamed 5Unnamed 6Unnamed 7Unnamed 8Unnamed 9Unnamed 10Unnamed 11Unnamed 12Unnamed 13Unnamed 14Unnamed 15Unnamed 16Unnamed 17Unnamed 18Unnamed 19Unnamed 20Unnamed 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(sents)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(words)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "metadata": {
        "id": "a0HBoMosalOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af7e524-5839-4431-f272-41a6379d7bfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['textlabelUnnamed 2Unnamed 3Unnamed 4Unnamed 5Unnamed 6Unnamed 7Unnamed 8Unnamed 9Unnamed 10Unnamed 11Unnamed 12Unnamed 13Unnamed 14Unnamed 15Unnamed 16Unnamed 17Unnamed 18Unnamed 19Unnamed 20Unnamed 21']\n",
            "The number of sentences is 1\n",
            "['textlabelUnnamed', '2Unnamed', '3Unnamed', '4Unnamed', '5Unnamed', '6Unnamed', '7Unnamed', '8Unnamed', '9Unnamed', '10Unnamed', '11Unnamed', '12Unnamed', '13Unnamed', '14Unnamed', '15Unnamed', '16Unnamed', '17Unnamed', '18Unnamed', '19Unnamed', '20Unnamed', '21']\n",
            "The number of tokens is 21\n",
            "The average number of tokens per sentence is 21\n",
            "The number of unique tokens are 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence) \n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word) \n",
        "        else:\n",
        "            unigram.append(word)\n",
        "    tokenized_text.append(sequence) \n",
        "    bigram.extend(list(ngrams(sequence, 2)))  \n",
        "#unigram, bigram, trigram, and fourgram models are created\n",
        "    trigram.extend(list(ngrams(sequence, 1)))\n",
        "    fourgram.extend(list(ngrams(sequence, 2)))"
      ],
      "metadata": {
        "id": "DgEPZCH-jQ0a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)"
      ],
      "metadata": {
        "id": "-XocgY4jje0C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = removal(bigram)\n",
        "trigram = removal(trigram)             \n",
        "fourgram = removal(fourgram)\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "freq_tri = nltk.FreqDist(trigram)\n",
        "freq_four = nltk.FreqDist(fourgram)\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
        "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8pkjJvgjiq1",
        "outputId": "d734beb7-368d-42ca-bb54-682a28378668"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams without stopword removal and without add-1 smoothing: \n",
            "\n",
            "Most common bigrams:  [(('textlabelunnamed', '2unnamed'), 1), (('2unnamed', '3unnamed'), 1), (('3unnamed', '4unnamed'), 1), (('4unnamed', '5unnamed'), 1), (('5unnamed', '6unnamed'), 1)]\n",
            "\n",
            "Most common trigrams:  [(('textlabelunnamed',), 1), (('2unnamed',), 1), (('3unnamed',), 1), (('4unnamed',), 1), (('5unnamed',), 1)]\n",
            "\n",
            "Most common fourgrams:  [(('textlabelunnamed', '2unnamed'), 1), (('2unnamed', '3unnamed'), 1), (('3unnamed', '4unnamed'), 1), (('4unnamed', '5unnamed'), 1), (('5unnamed', '6unnamed'), 1)]\n"
          ]
        }
      ]
    }
  ]
}